\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{statcourse}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}


\statcoursefinalcopy


\setcounter{page}{1}
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DO NOT EDIT ANYTHING ABOVE THIS LINE
% EXCEPT IF YOU LIKE TO USE ADDITIONAL PACKAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%% TITLE
\title{Arranging an Audio Track to other Genres\\
       by using CycleGAN-based Deep Learning Model 
       \thanks{Project proposal for Spring 2020, University of Wisconsin-Madison
               STAT453 Introduction to Deep Learning and Generative Models course (Instructor: Sebastian Raschka);\\
               All authors contributed equally}
}

\author{Alex DongHyeon Seo\\
{\tt\small dseo22@wisc.edu}
\and
Hyecheol (Jerry) Jang\\
{\tt\small hyecheol.jang@wisc.edu}
\and
Stella Kim\\
{\tt\small ykim736@wisc.edu}
}

\maketitle
%\thispagestyle{empty}



% MAIN ARTICLE GOES BELOW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%% ABSTRACT
\begin{abstract}
  Changing the genre of a song is one of the methods used when compositing music.
  To the best of our knowledge, musicians usually add their new ideas to the song,
  while trying to keep most of the special characteristics of the original song when arranging music.
  Similar to other artistic tasks that require human creativity,
  converting the genre of a song takes a significant amount of time and effort.
  In this project, we propose a method to translate a music genre by using machine-learning,
  which can generate a new song with comparably less amount of time than humans.
  Specifically, we utilized cycleGAN based model to translate a soundtrack to another soundtrack.\par
  Due to the complexity and difficulties of dealing with audio data,
  our model is able to handle files written with MIDI (Musical Instrument Digital Interface) specification only with
  specific characteristics.
  In the near future,
  we expect to expand our project to use regular audio files rather than MIDI to do our tasks to generalize our model.
  By doing so, we hope our model to be used for the general public without further modifications.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Recent advancements of Artificial Intelligence grant computers the abilities to mimic the noble creativity of the most
intellectual lives.
One of the advancements is computer-vision which allows computers to understand images just like human beings.
However, unlike images, music has not been studied as extensive in the field of deep learning.
Thus, we thought it would be interesting to do a project using data that represent audio.

Today, we are given many resources to share and access to all different types of music through apps like Spotify or
YouTube.
Due to those music apps and newly invented comfortable ear buds such as airPods, many people seem to be always listening
to music on a daily basis.
People listen to music while studying, driving, working and doing all other different types of tasks.
Because of such increasement in number of music listeners and people who are interested in music, musicians are always
trying hard to provide new musical experience to the listeners.
One of the ways that musicians create a song is changing a song to a different style of music while keeping the main
characteristics of the original song.
This is different from creating a totally new song from scratch and this way of changing a song to a new song by adding
some new ideas is called music arrangement.
Music arrangement can be done differently by making some different changes and one of the changes that could be made to
a song is the genre.
Only changing the genre of a song while keeping the main theme of the song might sound trivial but it is actually a very
difficult task which takes quite a long time and much effort.
Therefore, we would like to see if changing the genre of music could be done using machine-learning methods. 

Our goal is very similar to AmazonWeb Service’s DeepComposer \footnote{
  AWS DeepComposer Product Description Page: \url{https://aws.amazon.com/deepcomposer/}
},
a keyboard that allows people to create a new song or arrange to a different genre with
a simple combination of notes of their own.
DeepComposer is based on deep learning models using generator and discriminator to update the music.
Slightly different from DeepComposer,
our project will focus on transferring the genre of the song to the user’s desired genre.


\section{Literature Review}

\subsection{Audio Files}

% <TODO> Reason why we choose MIDI files
% First, we tried to deal with the binary audio files (MP3, WAV, AAC, and etc), but we were unable to find a good way to use audio files as an input source for the neural network. Specifically, extracting features (tones, pitch, and length of each sound) from the 
%     <TODO> appendix MIDI file specification

\subsection{Model Architecutures}

\subsection{Baseline Code}


\section{Proposed Method}

Describe the method(s) you are proposing, developing, or using. I.e., details
of the algorithms may be included here. 

\section{Experiments}

Describe the experiments you performed. You may want to create separate
subsections to further structure this section.


\subsection{Dataset}

Finding a large amount of music having paired labels(the arranged/mixed music based on the same track) is very
challenging and even considered as impossible task, since there is a limited number of arranged tracks for each
music(with high possibility, there is no arranged track).
To use cycleGAN~\cite{zhu2017unpaired}, we at least need to have unpaired labeled data.
More specifically, we need music sources which have genre information as their label.
Also, because the music industry is one of the most sensitive markets toward copyright issue,
it is also important to only use the music tracks which do not have any restrictions on the usage for research purposes.
\par
For this project, we collected MIDI files of different genres, including Classical, Pop, Rock, and Jazz.
We first tried collecting MIDI files of all four different genres from the same database to ensure the fairness and
equality across all the files; however, we could not find a good database that has good MIDI files of all the four
genres we intended to use for the project.
Thus, we allowed ourselves to collect the files of each genre from different databases.
At the end, we collected MIDI files of classical music from mfiles\footnote{
  Classical Database: \url{https://www.mfiles.co.uk/classical-midi.htm}
}
while MIDI files of Pop and Rock are from midiworld \footnote{
  Pop Database: \url{https://www.midiworld.com/search/?q=classic} \linebreak
  Rock Database: \url{https://www.midiworld.com/search/?q=rock}
}.
However, we had some difficulties finding a good set of MIDI files of Jazz music so we used the same data with
Brunner et al. \cite{brunner2018symbolic} 
\par
For classical, pop and rock MIDI files, we did data crawling since those files are from websites.
To do data crawling, we first used import.io, \footnote{
   Import.io product website: \url{https://www.import.io/}
}
which is a web application that gets all the text contents of a webpage including links.
Then we used R to cut off all the unnecessary text data we got from import.io to only select the download
links for files.
After cleaning up the data, then we used for-loop in R to go through the iterations and download all the files that are
linked to addresses that we found earlier.
For Jazz, we simply downloaded the files from GitHub.

% <TODO> Drawback of collecting files from different sources

% <TODO> Data pre-processing


\subsection{Software}

The baseline code was written in TensorFlow 1.4, whch is very out-dated.
We updated code so that it is at least runs without any warning or errors on the latest TensorFlow 1.x.
As of now, though the TensorFlow 1 is outdated, we decided to use TensorFlow 1 as we are more familiar to.
\par
To read and manipulate MIDI files properly, we utilizes 


\subsection{Hardware}

For local testing before uploading the code and start train our model,
we used Dell XPS 9550, which equipped Intel's i7-6700HQ @ 2.6Ghz, DDR4 2133Mhz,
and NVIDIA GeForce GTX 960M with vRAM of 2GB. \par
After testing locally, for our main training, we used Google's Colaboratory Pro.
We used standard instance (runtime),
which have 12GB of RAM with NVIDIA NVIDIA Tesla P100 Graphic Cards having vRAM of 16GB.


\section{Results and Discussion}

Describe the results you obtained from the experiments and interpret them.
Optionally, you could split "Results and Discussion" into two separate
sections.

\section{Conclusions}

Describe your conclusions here. If there are any future directions, you can
describe them here, or you can create a new section for future directions.


\section{Acknowledgements}

Our project is part of Spring 2020 semester's Statistics 453 \footnote{
  Course Website: \url{http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2020/} \linebreak
  GitHub Repository: \url{https://github.com/rasbt/stat453-deep-learning-ss20}
}
(Introduction to Deep Learning and Generative Models) course of the University of Wisconsin-Madison.
We specially appreciate with Prof. Raschka for all of his effort toward lectures and support toward our project.


\section{Contributions}

All of our team member decide to work on all steps and tasks together,
as we believe it is important for everyone to at least understand how and what we are doing to achieve the common goal.
However, as each individual has his/her own specialties and strength, some tasks has been designated to specific person.
\par
As Hyecheol (Jerry) Jang has strength on understanding and fixing the codes written on python,
he mainly worked on analyzing and fixing the baseline code.
To do so, he went through the tensorflow API to understand what's going on inside the code and to remove depressed and 
unsupported code parts from the baseline code.
\par
Alex studied similar studies and papers that are about creating music in various ways.
He did research on the papers to find out which method worked and why specific methods worked better than the others.
Also, he reviewed different models to find out which model would fit the best for the purpose of project. 
\par
Stella worked on finding the right MIDI files for the project.
She went through various websites and databases to select the correct MIDI files that could be used for the purpose of
our project.
Also, she worked on understanding the code of the baseline model for the data processing part,
to understand why the data was processed in such way, she had to study the MIDI file characteristics and music as well.
\par
Even though we had some specific tasks assigned to each person,
overall, as mentioned above, we helped each other and worked togehter on most of the tasks.
Also, we divided the work on the presentation and the report evenly as well.


{\small
\bibliographystyle{ieee}
\bibliography{bibliography.bib}
}

\end{document}
