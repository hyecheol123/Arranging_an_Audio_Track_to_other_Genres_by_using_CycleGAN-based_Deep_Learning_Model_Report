\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{statcourse}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}


\statcoursefinalcopy


\setcounter{page}{1}
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DO NOT EDIT ANYTHING ABOVE THIS LINE
% EXCEPT IF YOU LIKE TO USE ADDITIONAL PACKAGES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%% TITLE
\title{Arranging an Audio Track to other Genres\\
       by using CycleGAN-based Deep Learning Model 
       \thanks{Project proposal for Spring 2020, University of Wisconsin-Madison
               STAT453 Introduction to Deep Learning and Generative Models course (Instructor: Sebastian Raschka);\\
               All authors contributed equally}
}

\author{Alex DongHyeon Seo\\
{\tt\small dseo22@wisc.edu}
\and
Hyecheol (Jerry) Jang\\
{\tt\small hyecheol.jang@wisc.edu}
\and
Stella Kim\\
{\tt\small ykim736@wisc.edu}
}

\maketitle
%\thispagestyle{empty}



% MAIN ARTICLE GOES BELOW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%% ABSTRACT
\begin{abstract}
  Changing the genre of a song is one of the methods used when compositing music.
  To the best of our knowledge, musicians usually add their new ideas to the song,
  while trying to keep most of the special characteristics of the original song when arranging music.
  Similar to other artistic tasks that require human creativity,
  converting the genre of a song takes a significant amount of time and effort.
  In this project, we propose a method to translate a music genre by using machine-learning,
  which can generate a new song with comparably less amount of time than humans.
  Specifically, we utilized cycleGAN based model to translate a soundtrack to another soundtrack.\par
  Due to the complexity and difficulties of dealing with audio data,
  our model is able to handle files written with MIDI (Musical Instrument Digital Interface) specification only with
  specific characteristics.
  In the near future,
  we expect to expand our project to use regular audio files rather than MIDI to do our tasks to generalize our model.
  By doing so, we hope our model to be used for the general public without further modifications.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Recent advancements of Artificial Intelligence grant computers the abilities to mimic the noble creativity of the most
intellectual lives.
One of the advancements is computer-vision which allows computers to understand images just like human beings.
However, unlike images, music has not been studied as extensive in the field of deep learning.
Thus, we thought it would be interesting to do a project using data that represent audio.

Today, we are given many resources to share and access to all different types of music through apps like Spotify or
YouTube.
Due to those music apps and newly invented comfortable ear buds such as airPods, many people seem to be always listening
to music on a daily basis.
People listen to music while studying, driving, working and doing all other different types of tasks.
Because of such increasement in number of music listeners and people who are interested in music, musicians are always
trying hard to provide new musical experience to the listeners.
One of the ways that musicians create a song is changing a song to a different style of music while keeping the main
characteristics of the original song.
This is different from creating a totally new song from scratch and this way of changing a song to a new song by adding
some new ideas is called music arrangement.
Music arrangement can be done differently by making some different changes and one of the changes that could be made to
a song is the genre.
Only changing the genre of a song while keeping the main theme of the song might sound trivial but it is actually a very
difficult task which takes quite a long time and much effort.
Therefore, we would like to see if changing the genre of music could be done using machine-learning methods. 

Our goal is very similar to AmazonWeb Service’s DeepComposer \footnote{
  AWS DeepComposer Product Description Page: \url{https://aws.amazon.com/deepcomposer/}
},
a keyboard that allows people to create a new song or arrange to a different genre with
a simple combination of notes of their own.
DeepComposer is based on deep learning models using generator and discriminator to update the music.
Slightly different from DeepComposer,
our project will focus on transferring the genre of the song to the user’s desired genre.


\section{Literature Review / Backgrounds}

\subsection{Audio Files}

Our initial goal was to use raw audio files to train and test our model to generate a new song with a change in the
genre of the original song.
 
However, while trying to find any good examples of machine learning models that use binary audio files for training and 
testing, we realized that there is not a well-made model that could be used for raw audio files.
Unlike images, there are not many models that are made for audio files.
Thus, we decided to make a small change in our plan. 
\par 
Instead of using audio files directly to the model, since we could not find a model that takes audio files as input, 
we thoguht we could use a converter that translates audio files to MIDI files.
Even though we failed to find a machine learning model that uses audio files,
we were able to find some models that use MIDI files.
Because there are some models that were used with MIDI files,
we thought it would work out if we can translate audio files to MIDI files and use them to train the model. 
\par
However, we faced another difficulties.
We could not find the right converter that could be used in converting the type of file from audio such as MP3 to MIDI.
After failing to find a way to auido file directly to a machine learning model and also failing to find a good converter
that changes audio files into MIDI files,
we decided to use MIDI files and build a model that takes in MIDI files as input for training and testing. 
\par
\paragraph{MIDI Files}
MIDI file is a type of file that represents audio using binary based on 128 representations of different
instruments \cite{midiSpec}. 
Inside a MIDI file, there are instructions written in binary that describe the attributes and characteristics of the 
song.
For example, the file contains the information about the velocity of the song,
which instruments to be played at which point of the track time and any other details that are needed to make the music. 
\par
Some advantages of using MIDI files is that MIDI files specify the 128 different instruments independently.
As mentioned above, the MIDI files has their own way of representation of 128 instruments and they can be used in
various ways depending how they are written in the MIDI file.
Becuase those instruments are specifically described how they are played at which speed and time in MIDI files,
every details of the information about the instruments that make up the sound of songs could be recorded.
This is good for the purpose of our project since when generating a new song based on a song that already exists,
it is very important to keep most of the details and the main style of the song. 
\par
Therefore, having the specifications of the music is helpful in maintaining the important sounds and notes when making
a new song based off of the original version.


\subsection{Model Architecutures}

\subsection{Baseline Code}


\section{Proposed Method}

% Describe the method(s) you are proposing, developing, or using. I.e., details
% of the algorithms may be included here. 

\section{Experiments}

We generated two version of model to proof our idea.
The first model utilizes the original soundtrack without any filteration or merging.
The second model we tried merges all the channels in MIDI file to one track (representing the piano's sound).
\par
All the model follows the basic structures that proposed on Brunner et. al. \cite{brunner2018symbolic}.
For the second model, we tried four different settings during the training time.
The first setting uses learning rate of 0.0002,
epoch of 100 and second setting uses learing rate of 0.0001 and epoch of 200.
The first and second setting uses 64 convolution filters on generator.
For the third and fourth setting the learning rate and the epoch numbers are identical to the first and second setting, 
but it uses 128 convolution filters on generator. 


\subsection{Dataset}

Finding a large amount of music having paired labels(the arranged/mixed music based on the same track) is very
challenging and even considered as impossible task, since there is a limited number of arranged tracks for each
music(with high possibility, there is no arranged track).
To use cycleGAN~\cite{zhu2017unpaired}, we at least need to have unpaired labeled data.
More specifically, we need music sources which have genre information as their label.
Also, because the music industry is one of the most sensitive markets toward copyright issue,
it is also important to only use the music tracks which do not have any restrictions on the usage for research purposes.
\par
\paragraph{Data Collection}
For this project, we collected MIDI files of different genres, including Classical, Pop, Rock, and Jazz.
We first tried collecting MIDI files of all four different genres from the same database to ensure the fairness and
equality across all the files; however, we could not find a good database that has good MIDI files of all the four
genres we intended to use for the project.
Thus, we allowed ourselves to collect the files of each genre from different databases.
At the end, we collected MIDI files of classical music from mfiles\footnote{
  Classical Database: \url{https://www.mfiles.co.uk/classical-midi.htm}
}
while MIDI files of Pop and Rock are from midiworld \footnote{
  Pop Database: \url{https://www.midiworld.com/search/?q=classic} \linebreak
  Rock Database: \url{https://www.midiworld.com/search/?q=rock}
}.
However, we had some difficulties finding a good set of MIDI files of Jazz music so we used the same data with
Brunner et al. \cite{brunner2018symbolic} 
\par
For classical, pop and rock MIDI files, we did data crawling since those files are from websites.
To do data crawling, we first used import.io, \footnote{
   Import.io product website: \url{https://www.import.io/}
}
which is a web application that gets all the text contents of a webpage including links.
Then we used R to cut off all the unnecessary text data we got from import.io to only select the download
links for files.
After cleaning up the data, then we used for-loop in R to go through the iterations and download all the files that are
linked to addresses that we found earlier.
For Jazz, we simply downloaded the files from GitHub.
\par
\paragraph{Dataset Analysis}
After we downloaded the MIDI files, we manuver over the collected data to be more familiarized with those.
At a glance, as the MIDI files are usually generated by the amateurs, rather than officially released from professonal
entertainment companies, they are not well-organized.
For instance, Some of them contain meaningless trailing and ending whitespaces rather than their first note appears at
time 0.
\par
Moreover, as there exists the genres having characteristics from more than one genre
(for instance, pop rock has characteristics from both pop and rock musics),
it is hard to tell which musics belongs to which genre for some soundtrack.
Also, the existence of variations in one genre
(for example, inside of the rock genre, we have hard rock, progressive rock, punk rock and so on)
made us hard to pin-point the specific characteristics that each genre has.
However, as there is no available databases such that categorizes the MIDI files in very specific genres,
we decide to stay on using the big genre classes.
\par
\paragraph{Data Pre-Processing}
Due to the unstructuredness of the retrieved dataset, we are forced to preprocess the data.
During the data preprocessing, the baseline code dropped the songs which have time signiture change,
of which the first beat does not start at time zero, and of which time signiture is not 4/4 \cite{brunner2018symbolic}.
The filter that Brunner et. al used is implemented in a very precise and specific way,
which makes the songs that go through the filter ruins the song in such way that the msuic does not represent the 
intended genere anymore.
\par
For the first experiment to make the model more generalized, we removed those filters before we processed the data.
For the second experiment, which merges all the sound channels into one channel, we removed the trailing and ending
whitespaces so that it can minimize the blank audio slices after cutting the sound to fixed-length slices.


\subsection{Software}

The baseline code was written in TensorFlow 1.4, whch is very out-dated.
We updated code so that it is at least runs without any warning or errors on the latest TensorFlow 1.x.
As of now, though the TensorFlow 1 is outdated, we decided to use TensorFlow 1 as we are more familiar to.
\par
To read and manipulate MIDI files properly, we utilizes pretty\textunderscore midi \footnote{
  pretty\textunderscore midi Documentation: \url{http://craffel.github.io/pretty-midi/}
}
and Pypianoroll \footnote{
   Pypianoroll Documentation: \url{https://salu133445.github.io/pypianoroll/}
}.


\subsection{Hardware}

For local testing before uploading the code and start train our model,
we used Dell XPS 9550, which equipped Intel's i7-6700HQ @ 2.6Ghz, DDR4 2133Mhz,
and NVIDIA GeForce GTX 960M with vRAM of 2GB. \par
After testing locally, for our main training, we used Google's Colaboratory Pro.
We used standard instance (runtime),
which have 12GB of RAM with NVIDIA NVIDIA Tesla P100 Graphic Cards having vRAM of 16GB.


\section{Results and Discussion}

% Describe the results you obtained from the experiments and interpret them.
% Optionally, you could split "Results and Discussion" into two separate
% sections.

\section{Conclusions}

% Describe your conclusions here. If there are any future directions, you can
% describe them here, or you can create a new section for future directions.


\section{Acknowledgements}

Our project is part of Spring 2020 semester's Statistics 453 \footnote{
  Course Website: \url{http://pages.stat.wisc.edu/~sraschka/teaching/stat453-ss2020/} \linebreak
  GitHub Repository: \url{https://github.com/rasbt/stat453-deep-learning-ss20}
}
(Introduction to Deep Learning and Generative Models) course of the University of Wisconsin-Madison.
We specially appreciate with Prof. Raschka for all of his effort toward lectures and support toward our project.


\section{Contributions}

All of our team member decide to work on all steps and tasks together,
as we believe it is important for everyone to at least understand how and what we are doing to achieve the common goal.
However, as each individual has his/her own specialties and strength, some tasks has been designated to specific person.
\par
As Hyecheol (Jerry) Jang has strength on understanding and fixing the codes written on python,
he mainly worked on analyzing and fixing the baseline code.
To do so, he went through the tensorflow API to understand what's going on inside the code and to remove depressed and 
unsupported code parts from the baseline code.
\par
Alex studied similar studies and papers that are about creating music in various ways.
He did research on the papers to find out which method worked and why specific methods worked better than the others.
Also, he reviewed different models to find out which model would fit the best for the purpose of project. 
\par
Stella worked on finding the right MIDI files for the project.
She went through various websites and databases to select the correct MIDI files that could be used for the purpose of
our project.
Also, she worked on understanding the code of the baseline model for the data processing part,
to understand why the data was processed in such way, she had to study the MIDI file characteristics and music as well.
\par
Even though we had some specific tasks assigned to each person,
overall, as mentioned above, we helped each other and worked togehter on most of the tasks.
Also, we divided the work on the presentation and the report evenly as well.


\section{Codes}

All the codes are posted on the GitHub repository. \footnote{
   \url{https://github.com/hyecheol123/CycleGAN-Music-Style-Transfer}
}
The repository ipynb file that used as an interactive workspace to run the code.
All the data preprocessing codes are embedded on the notebook as we want to run those code only when needed.
The model has been implemented in the python files, and it is called inside the notebook.


{\small
\bibliographystyle{ieee}
\bibliography{bibliography.bib}
}

\end{document}
